<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="爬虫," />










<meta name="description" content="爬虫介绍互联网由许许多多的超文本（HTML）组成，用户利用HTTP或者HTTPS协议访问网页。通俗来讲，爬虫就是我们编程来模拟用户访问网页的过程。 聚焦爬虫的基本过程如下：  访问目标url。 解析数据。 获取待访问url，回到步骤1。 数据持久化，即将爬取的数据保存到本地。  urllib介绍urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用">
<meta name="keywords" content="爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫入门一">
<meta property="og:url" content="www.platot.site/2020/10/07/爬虫入门一/index.html">
<meta property="og:site_name" content="心远地自偏">
<meta property="og:description" content="爬虫介绍互联网由许许多多的超文本（HTML）组成，用户利用HTTP或者HTTPS协议访问网页。通俗来讲，爬虫就是我们编程来模拟用户访问网页的过程。 聚焦爬虫的基本过程如下：  访问目标url。 解析数据。 获取待访问url，回到步骤1。 数据持久化，即将爬取的数据保存到本地。  urllib介绍urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-10-11T14:15:37.889Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫入门一">
<meta name="twitter:description" content="爬虫介绍互联网由许许多多的超文本（HTML）组成，用户利用HTTP或者HTTPS协议访问网页。通俗来讲，爬虫就是我们编程来模拟用户访问网页的过程。 聚焦爬虫的基本过程如下：  访问目标url。 解析数据。 获取待访问url，回到步骤1。 数据持久化，即将爬取的数据保存到本地。  urllib介绍urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="www.platot.site/2020/10/07/爬虫入门一/"/>





  <title>爬虫入门一 | 心远地自偏</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">心远地自偏</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="www.platot.site/2020/10/07/爬虫入门一/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="plato">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="心远地自偏">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">爬虫入门一</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-10-07T21:54:27+08:00">
                2020-10-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/教程/" itemprop="url" rel="index">
                    <span itemprop="name">教程</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="爬虫介绍"><a href="#爬虫介绍" class="headerlink" title="爬虫介绍"></a>爬虫介绍</h3><p>互联网由许许多多的超文本（HTML）组成，用户利用HTTP或者HTTPS协议访问网页。通俗来讲，爬虫就是我们编程来模拟用户访问网页的过程。</p>
<p>聚焦爬虫的基本过程如下：</p>
<ol>
<li>访问目标url。</li>
<li>解析数据。</li>
<li>获取待访问url，回到步骤1。</li>
<li>数据持久化，即将爬取的数据保存到本地。</li>
</ol>
<h3 id="urllib介绍"><a href="#urllib介绍" class="headerlink" title="urllib介绍"></a>urllib介绍</h3><p>urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用到的子模块在Python3中的为urllib.request和urllib.parse。许多第三方的爬虫库都是基于urllib，比如Requests。</p>
<h3 id="最简单的爬虫"><a href="#最简单的爬虫" class="headerlink" title="最简单的爬虫"></a>最简单的爬虫</h3><p>下面是get请求的爬虫</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"http://www.baidu.com/"</span></span><br><span class="line">    <span class="comment"># 使用get请求获得相应</span></span><br><span class="line">    reponse = urllib.request.urlopen(url)</span><br><span class="line">    print(reponse)</span><br><span class="line">    data = reponse.read()</span><br><span class="line">    <span class="comment"># 使用UTF-8解码数据</span></span><br><span class="line">    str_data = data.decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(str_data)</span><br><span class="line">    <span class="comment"># 将数据保存到本地</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"baidu.html"</span>, <span class="string">"w"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(str_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    load_data()</span><br></pre></td></tr></table></figure>
<p>下面是post请求的爬虫，由于url只支持Ascii编码的字符，所以如果其包含中文字符，需要对其编码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">import string</span><br><span class="line"></span><br><span class="line"># 方法一：直接拼接，适合传少量参数时使用</span><br><span class="line">def get_method_params():</span><br><span class="line">    url = &quot;http://www.baidu.com/s?wd=&quot;</span><br><span class="line">    # 拼接字符串(汉字)</span><br><span class="line">    name = &apos;风景&apos;</span><br><span class="line">    final_url = url + name</span><br><span class="line">    print(final_url)</span><br><span class="line">    # 汉字没有转码</span><br><span class="line">    encode_new_url = urllib.parse.quote(final_url, safe=string.printable)</span><br><span class="line">    print(encode_new_url)</span><br><span class="line">    responses = urllib.request.urlopen(encode_new_url)</span><br><span class="line">    data = responses.read().decode(&apos;utf-8&apos;)</span><br><span class="line">    # 保存到本地</span><br><span class="line">    with open(&apos;02-encode.html&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">        f.write(data)</span><br><span class="line"></span><br><span class="line"># 方法二：使用字典，适合大量参数时</span><br><span class="line">def get_params():</span><br><span class="line">    url = &apos;http://www.baidu.com/s?&apos;</span><br><span class="line">    params = &#123;&quot;wd&quot;:&quot;中文&quot;,</span><br><span class="line">              &quot;key&quot;:&quot;zhang&quot;,</span><br><span class="line">              &quot;value&quot;:&quot;san&quot;</span><br><span class="line">              &#125;</span><br><span class="line">    str_params = urllib.parse.urlencode(params)</span><br><span class="line">    print(str_params)</span><br><span class="line">    final_url = url + str_params</span><br><span class="line">    end_url = urllib.parse.quote(final_url, safe=string.printable)</span><br><span class="line">    response = urllib.request.urlopen(end_url)</span><br><span class="line">    data = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">    print(data)</span><br><span class="line">    </span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    get_method_params()</span><br><span class="line">    get_params()</span><br></pre></td></tr></table></figure>
<h3 id="请求头和代理"><a href="#请求头和代理" class="headerlink" title="请求头和代理"></a>请求头和代理</h3><h4 id="添加请求头"><a href="#添加请求头" class="headerlink" title="添加请求头"></a>添加请求头</h4><p>许多网站直接通过请求头，判别一次访问是由浏览器还是爬虫程序发起的，这主要是通过User-Agent字段，下面显示如何给我们的爬虫添加请求头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_baidu</span><span class="params">()</span>:</span></span><br><span class="line">    url= <span class="string">"https://www.baidu.com"</span></span><br><span class="line">    header = &#123;</span><br><span class="line">        <span class="comment">#浏览器的版本</span></span><br><span class="line">        <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36"</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#创建请求对象</span></span><br><span class="line">    request = urllib.request.Request(url)</span><br><span class="line">    <span class="comment"># 构建请求对象时添加header信息</span></span><br><span class="line">    <span class="comment"># request = urllib.request.Request(url, headers=header)</span></span><br><span class="line">    <span class="comment">#动态的去添加head的信息</span></span><br><span class="line">    request.add_header(<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36"</span>)</span><br><span class="line">    <span class="comment">#请求网络数据(不在此处增加请求头信息因为此方法系统没有提供参数)</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    print(response)</span><br><span class="line">    data = response.read().decode(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取到完整的url</span></span><br><span class="line">    final_url = request.get_full_url()</span><br><span class="line">    print(final_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#响应头</span></span><br><span class="line">    <span class="comment"># print(response.headers)</span></span><br><span class="line">    <span class="comment">#获取请求头的信息(所有的头的信息)</span></span><br><span class="line">    <span class="comment"># request_headers = request.headers</span></span><br><span class="line">    <span class="comment"># print(request_headers)</span></span><br><span class="line">    <span class="comment">#(2)第二种方式打印headers的信息</span></span><br><span class="line">    <span class="comment">#注意点:首字母需要大写,其他字母都小写</span></span><br><span class="line">    request_headers = request.get_header(<span class="string">"User-agent"</span>)</span><br><span class="line">    <span class="comment"># print(request_headers)</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"02header.html"</span>,<span class="string">"w"</span>)<span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    load_baidu()</span><br></pre></td></tr></table></figure>
<h4 id="设置代理服务器"><a href="#设置代理服务器" class="headerlink" title="设置代理服务器"></a>设置代理服务器</h4><p>有些网站会通过限制ip的访问频率来反爬虫，我们可以通过代理来通过这一限制，所谓代理就是我们通过中间服务器去访问网页，中间服务器再将访问结果传给我们。代理分为三种：</p>
<ol>
<li>透明。目标主机知道这是代理，也知道我们主机的ip。</li>
<li>匿名。目标主机知道这是代理，但不知道我们主机的ip。</li>
<li>高匿。目标主机不知道这是代理，也不知道我们主机的ip。</li>
</ol>
<p>下面演示免费代理的设置方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_proxy_handler</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">"https://blog.csdn.net/m0_37499059/article/details/79003731"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#添加代理</span></span><br><span class="line">    proxy = &#123;</span><br><span class="line">        <span class="comment">#免费代理的写法</span></span><br><span class="line">        <span class="string">"http"</span>:<span class="string">"120.77.249.46:8080"</span>,</span><br><span class="line">        <span class="comment">#付费代理的写法</span></span><br><span class="line">        <span class="comment"># "http":"username:pwd@115.123.23.2“</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#代理处理器</span></span><br><span class="line">    proxy_handler = urllib.request.ProxyHandler(proxy)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建自己opener</span></span><br><span class="line">    opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">    <span class="comment">#拿着代理ip去发送请求</span></span><br><span class="line">    response = opener.open(url)</span><br><span class="line">    data = response.read().decode(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"03header.html"</span>, <span class="string">"w"</span>)<span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    create_proxy_handler()</span><br></pre></td></tr></table></figure>
<p>下面演示如何设置付费代理。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">#付费的代理发送</span><br><span class="line">#1.用户名密码(带着)</span><br><span class="line">#通过验证的处理器来发送</span><br><span class="line"></span><br><span class="line">def money_proxy_use():</span><br><span class="line">    # #第一种方式付费代理发送请求</span><br><span class="line">    # #1.代理ip</span><br><span class="line">    # money_proxy =&#123;&quot;http&quot;:&quot;username:pwd@192.168.12.11:8080&quot;&#125;</span><br><span class="line">    # #2.代理的处理器</span><br><span class="line">    # proxy_handler=urllib.request.ProxyHandler(money_proxy)</span><br><span class="line">    #</span><br><span class="line">    # #3.通过处理器创建opener</span><br><span class="line">    # opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">    # #4.open发送请求</span><br><span class="line">    # opener.open(&quot;http://www.baidu.com&quot;)</span><br><span class="line">    # #第二种方式发送付费的ip地址</span><br><span class="line">    use_name = &quot;abcname&quot;</span><br><span class="line">    pwd = &quot;123456&quot;</span><br><span class="line">    proxy_money = &quot;123.158.63.130:8888&quot;</span><br><span class="line">    #2.创建密码管理器,添加用户名和密码</span><br><span class="line">    password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">    #uri定位 uri&gt;url</span><br><span class="line">    #url 资源定位符</span><br><span class="line">    password_manager.add_password(None,proxy_money,use_name,pwd)</span><br><span class="line">    #3.创建可以验证代理ip的处理器</span><br><span class="line">    handle_auth_proxy = urllib.request.ProxyBasicAuthHandler(password_manager)</span><br><span class="line">    #4.根据处理器创建opener</span><br><span class="line">    opener_auth = urllib.request.build_opener(handle_auth_proxy)</span><br><span class="line">    #5.发送请求</span><br><span class="line">    response = opener_auth.open(&quot;http://www.baidu.com&quot;)</span><br><span class="line">    print(response.read())</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    money_proxy_use()</span><br></pre></td></tr></table></figure>
<h4 id="爬取免费代理"><a href="#爬取免费代理" class="headerlink" title="爬取免费代理"></a>爬取免费代理</h4><p>下面演示如何从免费代理网站爬取ip并验证有效性的程序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> http.client <span class="keyword">import</span> responses</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ip_pool = []</span><br><span class="line"><span class="comment"># 爬取代理ip</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ips</span><span class="params">(url, n=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        current_url = url  + str(j) +<span class="string">"/"</span></span><br><span class="line">        header = &#123;<span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) "</span>,</span><br><span class="line">  <span class="string">"Accept"</span> : <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span>&#125;</span><br><span class="line"></span><br><span class="line">        response = requests.get(current_url, headers=header)</span><br><span class="line">        print(response.status_code)</span><br><span class="line">        html = response.text</span><br><span class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> soup.find_all(<span class="string">'tr'</span>):</span><br><span class="line">            <span class="keyword">if</span> tr.find(<span class="string">'td'</span>) == <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            ip = tr.find(attrs=&#123;<span class="string">'data-title'</span>:<span class="string">'IP'</span>&#125;).get_text()</span><br><span class="line">            port = tr.find(attrs=&#123;<span class="string">'data-title'</span>:<span class="string">'PORT'</span>&#125;).get_text()</span><br><span class="line">            typ = tr.find(attrs=&#123;<span class="string">'data-title'</span>:<span class="string">"类型"</span>&#125;).get_text()</span><br><span class="line">            ip_pool.append(&#123;typ.lower():ip + <span class="string">":"</span> + port&#125;)</span><br><span class="line"></span><br><span class="line">valiation_url = <span class="string">'https://blog.csdn.net/m0_37499059/article/details/79003731'</span></span><br><span class="line">useragents = [</span><br><span class="line">                <span class="string">'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'</span>,</span><br><span class="line">                <span class="string">'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0'</span>,</span><br><span class="line">                <span class="string">'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'</span>,</span><br><span class="line">                <span class="string">'Mozilla/5.0 (compatible; WOW64; MSIE 10.0; Windows NT 6.2)'</span>,</span><br><span class="line">                <span class="string">'Opera/9.80 (Windows NT 6.1; WOW64; U; en) Presto/2.10.229 Version/11.62'</span>,</span><br><span class="line">                <span class="string">'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; en-US) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27'</span></span><br><span class="line">                ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    user_agent = random.choice(useragents)</span><br><span class="line">    ip = random.choice(ip_pool)</span><br><span class="line">    <span class="keyword">return</span> user_agent, ip</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_proxy_opener</span><span class="params">(ip)</span>:</span></span><br><span class="line">    handler = urllib.request.ProxyHandler(ip)</span><br><span class="line">    opener = urllib.request.build_opener(handler)</span><br><span class="line">    <span class="keyword">return</span> opener</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valiation_proxy</span><span class="params">(ip_pool)</span>:</span></span><br><span class="line">    pools = []</span><br><span class="line">    <span class="keyword">for</span> ip <span class="keyword">in</span> ip_pool:</span><br><span class="line">        request = urllib.request.Request(valiation_url, headers=&#123;<span class="string">"User-Agent"</span>:<span class="string">'Mozilla/5.0 (compatible; WOW64; MSIE 10.0; Windows NT 6.2)'</span>&#125;)</span><br><span class="line">        <span class="comment"># request.add_header("User-Agent", user_agent)</span></span><br><span class="line">        opener = create_proxy_opener(ip)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">            response = opener.open(request)</span><br><span class="line">            pools.append(ip)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pools</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.kuaidaili.com/free/inha/'</span></span><br><span class="line">get_ips(url, <span class="number">50</span>)</span><br><span class="line">print(ip_pool)</span><br><span class="line">ip_pools = valiation_proxy(ip_pool)</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = json.dumps(ip_pools)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"ip_pools.npz"</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(data)</span><br><span class="line">print(ip_pools)</span><br></pre></td></tr></table></figure>
<h4 id="HTTP认证"><a href="#HTTP认证" class="headerlink" title="HTTP认证"></a>HTTP认证</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://ssr3.scrape.center/'</span></span><br><span class="line"><span class="comment"># 构建一个密码管理对象，用来保存需要处理的用户名和密码</span></span><br><span class="line">passwordmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line"><span class="comment"># 指定 url 、username 、password</span></span><br><span class="line">passwordmgr.add_password(<span class="keyword">None</span>, url, <span class="string">'admin'</span>, <span class="string">'admin'</span>)</span><br><span class="line"><span class="comment"># 构建一个用户名/密码验证的处理器对象</span></span><br><span class="line">handler = urllib.request.HTTPBasicAuthHandler(passwordmgr)</span><br><span class="line"><span class="comment"># 创建opener对象，这个opener对象在发送请求时就相当于验证成功了</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.open(url)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h4 id="处理cookies"><a href="#处理cookies" class="headerlink" title="处理cookies"></a>处理cookies</h4><p>下面演示如何登录网站，并保存下cookies</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 代码登录</span></span><br><span class="line"><span class="comment"># 1.1 登录网址</span></span><br><span class="line">log_url = <span class="string">'https://www.yaozh.com/login'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.2 登录上传的参数</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'username'</span>: <span class="string">'*****'</span>,</span><br><span class="line">    <span class="string">'pwd'</span>: <span class="string">'******'</span>,</span><br><span class="line">    <span class="string">'formhash'</span>: <span class="string">'16364D7F2C'</span>,</span><br><span class="line">    <span class="string">'backurl'</span>: <span class="string">r'https%3A%2F%2Fwww.yaozh.com%2F'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数转码</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造request请求</span></span><br><span class="line"><span class="comment"># 添加请求头</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User=Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'www.yaozh.com'</span>, </span><br><span class="line">    <span class="string">'Origin'</span>: <span class="string">'https://www.yaozh.com'</span>,</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">'https://www.yaozh.com/login/proxy?time=1602222533216'</span></span><br><span class="line">&#125;</span><br><span class="line">request = urllib.request.Request(log_url, headers=headers, data=data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># http.cookiejar 模块定义了用于自动处理 HTTP cookie 的类</span></span><br><span class="line">cookie_jar = cookiejar.CookieJar()</span><br><span class="line"><span class="comment"># 定义有添加cookie功能的处理器</span></span><br><span class="line">cookie_handler = urllib.request.HTTPCookieProcessor(cookie_jar)</span><br><span class="line"><span class="comment"># 根据处理器生成opener</span></span><br><span class="line">opener = urllib.request.build_opener(cookie_handler)</span><br><span class="line"><span class="comment"># 带着参数，发送post请求</span></span><br><span class="line"><span class="comment"># 如果登录成功，cookiejar自动保存cookie</span></span><br><span class="line">responese = opener.open(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 代码带着cookie去访问个人中心</span></span><br><span class="line">center_url = <span class="string">'https://www.yaozh.com/member/'</span></span><br><span class="line">request = urllib.request.Request(center_url, headers=headers)</span><br><span class="line">response = opener.open(request)</span><br><span class="line">data= response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"02cookie.html"</span>, <span class="string">"w"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(data)</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/27/Multi-Class Imbalanced Graph Convolutional Network Learning/" rel="next" title="Multi-Class Imbalanced Graph Convolutional Network Learning">
                <i class="fa fa-chevron-left"></i> Multi-Class Imbalanced Graph Convolutional Network Learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/10/10/Unsupervised Differentiable Multi-aspect Network Embedding/" rel="prev" title="Unsupervised Differentiable Multi-aspect Network Embedding">
                Unsupervised Differentiable Multi-aspect Network Embedding <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">plato</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">46</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫介绍"><span class="nav-number">1.</span> <span class="nav-text">爬虫介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urllib介绍"><span class="nav-number">2.</span> <span class="nav-text">urllib介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最简单的爬虫"><span class="nav-number">3.</span> <span class="nav-text">最简单的爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#请求头和代理"><span class="nav-number">4.</span> <span class="nav-text">请求头和代理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#添加请求头"><span class="nav-number">4.1.</span> <span class="nav-text">添加请求头</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#设置代理服务器"><span class="nav-number">4.2.</span> <span class="nav-text">设置代理服务器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬取免费代理"><span class="nav-number">4.3.</span> <span class="nav-text">爬取免费代理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTP认证"><span class="nav-number">4.4.</span> <span class="nav-text">HTTP认证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#处理cookies"><span class="nav-number">4.5.</span> <span class="nav-text">处理cookies</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">plato</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
